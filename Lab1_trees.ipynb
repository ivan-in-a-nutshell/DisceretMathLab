{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# !pip install networkx\n",
    "# !pip install matplotlib\n",
    "# !pip install tqdm\n",
    "# !pip install pandas\n",
    "# !pip install numpy\n",
    "# !pip install graphviz\n",
    "# !pip install scikit-learn"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import random\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import combinations, groupby\n",
    "import pandas as pd\n",
    "\n",
    "from networkx.algorithms import tree\n",
    "from networkx.algorithms import bellman_ford_predecessor_and_distance\n",
    "from networkx.algorithms import floyd_warshall_predecessor_and_distance\n",
    "\n",
    "import numpy.typing as npt\n",
    "\n",
    "import heapq\n",
    "from tqdm import tqdm\n",
    "\n",
    "import time"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Task 1. Algorithm's analysis"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Generating graph"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "# You can use this function to generate a random graph with 'num_of_nodes' nodes\n",
    "# and 'completeness' probability of an edge between any two nodes\n",
    "# If 'directed' is True, the graph will be directed\n",
    "# If 'draw' is True, the graph will be drawn\n",
    "def gnp_random_connected_graph(num_of_nodes: int,\n",
    "                               completeness: int,\n",
    "                               directed: bool = False,\n",
    "                               draw: bool = False):\n",
    "    \"\"\"\n",
    "    Generates a random graph, similarly to an Erdős-Rényi \n",
    "    graph, but enforcing that the resulting graph is conneted (in case of undirected graphs)\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    if directed:\n",
    "        G = nx.DiGraph()\n",
    "    else:\n",
    "        G = nx.Graph()\n",
    "    edges = combinations(range(num_of_nodes), 2)\n",
    "    G.add_nodes_from(range(num_of_nodes))\n",
    "    \n",
    "    for _, node_edges in groupby(edges, key = lambda x: x[0]):\n",
    "        node_edges = list(node_edges)\n",
    "        random_edge = random.choice(node_edges)\n",
    "        if random.random() < 0.5:\n",
    "            random_edge = random_edge[::-1]\n",
    "        G.add_edge(*random_edge)\n",
    "        for e in node_edges:\n",
    "            if random.random() < completeness:\n",
    "                G.add_edge(*e)\n",
    "                \n",
    "    for (u,v,w) in G.edges(data=True):\n",
    "        w['weight'] = random.randint(-5, 20)\n",
    "                \n",
    "    if draw: \n",
    "        plt.figure(figsize=(10,6))\n",
    "        if directed:\n",
    "            # draw with edge weights\n",
    "            pos = nx.arf_layout(G)\n",
    "            nx.draw(G,pos, node_color='lightblue', \n",
    "                    with_labels=True,\n",
    "                    node_size=500, \n",
    "                    arrowsize=20, \n",
    "                    arrows=True)\n",
    "            labels = nx.get_edge_attributes(G,'weight')\n",
    "            nx.draw_networkx_edge_labels(G, pos,edge_labels=labels)\n",
    "            \n",
    "        else:\n",
    "            nx.draw(G, node_color='lightblue', \n",
    "                with_labels=True, \n",
    "                node_size=500)\n",
    "        \n",
    "    return G"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "G = gnp_random_connected_graph(20, 0.5, False, False)"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Subtask 1.1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Дослідження алгоритмів Крускала та Прима. Алгоритми будують каркаси простого зваженого графа"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Kruskal's algorithm"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "mstk = tree.minimum_spanning_tree(G, algorithm=\"kruskal\")"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "nx.draw(mstk, node_color='lightblue', \n",
    "        with_labels=True, \n",
    "        node_size=500)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "len(mstk.edges()), sum(el[2] for el in mstk.edges.data\n",
    "('weight'))"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Алгоритм Крускала. Алгоритм сортує всі ребра за вагою, обирає найменше та додає до каркасу,\n",
    "перевіривши чи не утвориться циклів. Алгоритм має часову складність O(E*log(V)), де E - кількість\n",
    " ребер, V -  кількість вершин"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def kruskals_alg(G: nx.Graph) -> nx.Graph:\n",
    "    \"\"\"Find the minimum spanning tree of a given graph using Kruskal's algorithm\"\"\"\n",
    "    min_span_tree = nx.Graph()\n",
    "    forest = []\n",
    "    for v in G.nodes():\n",
    "        forest.append({v})\n",
    "\n",
    "    for u, v, w in sorted(G.edges.data('weight'), key=lambda x: x[2]):\n",
    "        for i, s in enumerate(forest):\n",
    "            if u in s:\n",
    "                break\n",
    "        for j, s in enumerate(forest):\n",
    "            if v in s:\n",
    "                break\n",
    "        if i != j:\n",
    "            min_span_tree.add_edge(u, v, weight=w)\n",
    "            forest[i] |= forest[j]\n",
    "            forest.pop(j)\n",
    "    return nx.Graph(min_span_tree)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "min_span_tree = kruskals_alg(G)"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "nx.draw(min_span_tree, node_color='lightblue',\n",
    "        with_labels=True,\n",
    "        node_size=200)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "len(min_span_tree.edges()), sum(el[2] for el in min_span_tree.edges.data\n",
    "('weight'))"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Prim's algorithm"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "mstp = tree.minimum_spanning_tree(G, algorithm=\"prim\")"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "nx.draw(mstp, node_color='lightblue', \n",
    "        with_labels=True, \n",
    "        node_size=500)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "len(mstp.edges()), sum(el[2] for el in mstp.edges.data('weight'))"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Алгоритм Прима. Вибравши довільну першу вершину, алгоритм додає нові інцидентні ребра до вже\n",
    "доданих вершин з найменшою вагою. Складність алгоритму O(V^2) де V - кількість вершин, проте якщо\n",
    " використовувати binary heap queue, то складність стає O(E*log(V)) де E - кількість ребер, отже\n",
    " складність така ж як у алгоритму Крускала"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def prims_alg(G: nx.Graph) -> nx.Graph:\n",
    "    \"\"\"Find the minimum spanning tree of a given graph using Prim's algorithm\"\"\"\n",
    "    min_span_tree = nx.Graph()\n",
    "    heap = []\n",
    "    visited = set()\n",
    "    for v, w in G[0].items():\n",
    "        heapq.heappush(heap, (w['weight'], 0, v))\n",
    "    while heap:\n",
    "        w, v, u = heapq.heappop(heap)\n",
    "        if u in visited:\n",
    "            continue\n",
    "        min_span_tree.add_edge(v, u, weight=w)\n",
    "        visited.add(u)\n",
    "\n",
    "        for neighbor in G.neighbors(u):\n",
    "            if neighbor not in visited:\n",
    "                heapq.heappush(heap, (G[u][neighbor]['weight'], u, neighbor))\n",
    "    return min_span_tree"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "tree = prims_alg(G)"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "nx.draw(tree, node_color='lightblue',\n",
    "        with_labels=True,\n",
    "        node_size=500)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "len(tree.edges()), sum(el[2] for el in tree.edges.data('weight'))"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def measure_time(graphs: list[int], iter: int) -> tuple[list]:\n",
    "    \"\"\"Measure time of all algs\"\"\"\n",
    "    times = [[0]*7,[0]*7,[0]*7,[0]*7]\n",
    "    for i, num_of_nodes in enumerate(graphs):\n",
    "        for _ in tqdm(range(iter)):\n",
    "            graph = gnp_random_connected_graph(num_of_nodes, 0.8, False, False)\n",
    "\n",
    "            start = time.perf_counter()\n",
    "            kruskals_alg(graph)\n",
    "            times[0][i] += time.perf_counter() - start\n",
    "\n",
    "            start = time.perf_counter()\n",
    "            prims_alg(graph)\n",
    "            times[1][i] += time.perf_counter() - start\n",
    "\n",
    "            start = time.perf_counter()\n",
    "            tree.minimum_spanning_tree(graph, algorithm=\"kruskal\")\n",
    "            times[2][i] += time.perf_counter() - start\n",
    "\n",
    "            start = time.perf_counter()\n",
    "            tree.minimum_spanning_tree(graph, algorithm=\"prim\")\n",
    "            times[3][i] += time.perf_counter() - start\n",
    "\n",
    "    return [[el/iter for el in el_time] for el_time in times]\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "x = [10, 20, 50, 100, 200, 500, 1000]\n",
    "y1, y2, y3, y4 = measure_time(x, 100)\n",
    "plt.plot(x, y1, label='Kruskal\\'s')\n",
    "plt.plot(x, y2, label='Prim\\'s')\n",
    "plt.plot(x, y3, label='Built-in Kruskal\\'s')\n",
    "plt.plot(x, y4, label='Built-in Prim\\'s')\n",
    "plt.xlabel('Number of nodes')\n",
    "plt.ylabel('Time (seconds)')\n",
    "plt.legend()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "На графіках видно як час алгоритмів зростає з кількістю вершин. Причому цікаво те, що алгоритми\n",
    "вбудовані у бібліотеці networkx на великих графах працюють повільніше ніж ті що були\n",
    "імплементовані мною."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Subtask 1.2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "G = gnp_random_connected_graph(10, 0.5, True, True)"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Bellman-Ford algorithm"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# pred is a dictionary of predecessors, dist is a dictionary of distances\n",
    "try:\n",
    "    pred, dist = bellman_ford_predecessor_and_distance(G, 0)\n",
    "    for k, v in dist.items():\n",
    "        print(f\"Distance to {k}:\", v)\n",
    "except:\n",
    "    print(\"Negative cycle detected\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "*put your code below* (delete this)"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Floyd-Warshall algorithm"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# pred is a dictionary of predecessors, dist is a dictionary of distances dictionaries\n",
    "try:\n",
    "    pred, dist = floyd_warshall_predecessor_and_distance(G) \n",
    "    for k, v in dist.items():\n",
    "        print(f\"Distances with {k} source:\", dict(v))\n",
    "except:\n",
    "    print(\"Negative cycle detected\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "*put your code below* (delete this)"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "---"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Some useful explanations\n",
    "### How to get list of edges for your algorithm"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "edges = list(G.edges()) # by default G.edges are EdgesView class"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "edges[:5]"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### To get edges with weights"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "edges = list(G.edges(data=True))"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "edges[:5]"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "nodes = list(G.nodes())\n",
    "print(nodes)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Example on time measuring\n",
    "\n",
    "Read more on this: https://realpython.com/python-timer/\n",
    "\n",
    "Recall that you should measure times for 5, 10, 20, 50, 100, 200, 500 nodes 1000 times (and take mean of time taken for each node amount).\n",
    "\n",
    "Then you should build the plot for two algorithms (x - data size, y - mean time of execution)."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import time\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "NUM_OF_ITERATIONS = 1000\n",
    "time_taken = 0\n",
    "for i in tqdm(range(NUM_OF_ITERATIONS)):\n",
    "    \n",
    "    # note that we should not measure time of graph creation\n",
    "    G = gnp_random_connected_graph(100, 0.4, False)\n",
    "    \n",
    "    start = time.time()\n",
    "    tree.minimum_spanning_tree(G, algorithm=\"prim\")\n",
    "    end = time.time()\n",
    "    \n",
    "    time_taken += end - start\n",
    "\n",
    "time_taken / NUM_OF_ITERATIONS"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Task 2. Decision Tree Classifier "
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-07T16:27:38.967635Z",
     "start_time": "2025-02-07T16:27:38.964239Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# scikit-learn package\n",
    "from sklearn.datasets import load_iris, load_breast_cancer, load_diabetes\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import train_test_split"
   ],
   "outputs": [],
   "execution_count": 53
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "![purple-divider](https://user-images.githubusercontent.com/7065401/52071927-c1cd7100-2562-11e9-908a-dde91ba14e59.png)"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### General idea\n",
    "\n",
    "#### You are expected to write a quite simple, yet good core logic of decision tree classifier class. Additionaly, you need to test your results and write down a report on what you've done, which principles used and explain the general process.\n",
    "\n",
    "#### Hopefully, you have already learned what is decision tree classifier and how it work. For better understanding, and in case if something is still unclear for you, here are some useful links on basics of DTC:\n",
    "- https://www.youtube.com/watch?v=ZVR2Way4nwQ\n",
    "- https://towardsdatascience.com/decision-tree-classifier-explained-a-visual-guide-with-code-examples-for-beginners-7c863f06a71e\n",
    "- https://towardsdatascience.com/decision-tree-from-scratch-in-python-46e99dfea775\n",
    "- https://www.kaggle.com/code/prashant111/decision-tree-classifier-tutorial\n",
    "- https://towardsdatascience.com/decision-tree-classifier-explained-in-real-life-picking-a-vacation-destination-6226b2b6057\n",
    "\n",
    "#### Also, for those interested to learn more about machine learning and particulary Desicion Trees - here is a great course on Coursera (you may be interested in the whole course or just this particular week):\n",
    "- https://www.coursera.org/learn/advanced-learning-algorithms/home/week/4\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "![purple-divider](https://user-images.githubusercontent.com/7065401/52071927-c1cd7100-2562-11e9-908a-dde91ba14e59.png)"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "### Dataset\n",
    "#### You can use Iris dataset for this task. It is a very popular dataset for machine learning and data science. It contains 150 samples of 3 different species of Iris flowers (Iris setosa, Iris virginica and Iris versicolor). Four features were measured from each sample: the length and the width of the sepals and petals, in centimeters. \n",
    "Read more on this: https://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html\n",
    "https://en.wikipedia.org/wiki/Iris_flower_data_set\n",
    "#### However, using more interesting and intricate datasets is much appreciated. You can use any dataset you want, but it should be a classification one. For example you can use breast cancer or wine datasets, which are also available in sklearn.datasets. Or you can use any other dataset you find interesting.\n",
    "P.S. In case you are not sure if your dataset is suitable, feel free to ask assistants :)."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "![purple-divider](https://user-images.githubusercontent.com/7065401/52071927-c1cd7100-2562-11e9-908a-dde91ba14e59.png)"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-07T16:13:52.964470Z",
     "start_time": "2025-02-07T16:13:52.958404Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load dataset\n",
    "iris = load_iris()\n",
    "dir(iris)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DESCR',\n",
       " 'data',\n",
       " 'data_module',\n",
       " 'feature_names',\n",
       " 'filename',\n",
       " 'frame',\n",
       " 'target',\n",
       " 'target_names']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 44
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "iris.data.shape"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This means that we have 150 entries (samples, infos about a flower). The columns being: Sepal Length, Sepal Width, Petal Length and Petal Width(features). Let's look at first two entries:"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "iris.data[0:2]"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### To undestand data little bit better, let's plot some features"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "X = iris.data[:, :2]  # we only take the first two features.\n",
    "y = iris.target\n",
    "\n",
    "plt.figure(2, figsize=(8, 6))\n",
    "plt.clf()\n",
    "\n",
    "# Plot the training points\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Set1, edgecolor=\"k\")\n",
    "plt.xlabel(\"Sepal length\")\n",
    "plt.ylabel(\"Sepal width\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "From this we can clearly see, that even basing on those two parameters, we can clearly divide (classify) out data into several groups. For this, we will use decision tree classifier: https://scikit-learn.org/stable/modules/tree.html#tree\n",
    "\n",
    "![purple-divider](https://user-images.githubusercontent.com/7065401/52071927-c1cd7100-2562-11e9-908a-dde91ba14e59.png)\n",
    "\n",
    "### Example of usage\n",
    "\n",
    "\n",
    "**Decision Trees (DTs) are a non-parametric supervised learning method used for classification and regression**. The goal is to create a model that predicts the value of a target variable by learning simple decision rules inferred from the data features. A tree can be seen as a piecewise constant approximation."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-07T16:13:58.122162Z",
     "start_time": "2025-02-07T16:13:58.118032Z"
    }
   },
   "cell_type": "code",
   "source": [
    "clf = DecisionTreeClassifier()\n",
    "\n",
    "X, y = iris.data, iris.target\n",
    "X.shape, y.shape"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((150, 4), (150,))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 45
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Train / test split\n",
    "\n",
    "We train our model using training dataset and evaluate its performance basing on the test dataset. Reason to use two separate datasets is that our model learns its parameters from data, thus test set allows us to check its possibilities on completely new data."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "X, X_test, y, y_test = train_test_split(X, y, test_size= 0.20)\n",
    "X_test.shape, y_test.shape"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Model learning\n",
    "\n",
    "It learns its parameters (where it should split data and for what threshold value) basing on the training dataset. It is done by minimizing some cost function (e.g. Gini impurity or entropy)."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "clf = clf.fit(X, y)"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Visualization of produced tree\n",
    "\n",
    "You do not need to understand this piece of code :)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import graphviz \n",
    "dot_data = tree.export_graphviz(clf, out_file=None) \n",
    "graph = graphviz.Source(dot_data) \n",
    "graph.render(\"iris\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "dot_data = tree.export_graphviz(clf, out_file=None, \n",
    "                     feature_names=iris.feature_names,  \n",
    "                     class_names=iris.target_names,  \n",
    "                     filled=True, rounded=True,  \n",
    "                     special_characters=True)  \n",
    "graph = graphviz.Source(dot_data)  \n",
    "graph "
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Prediction step\n",
    "\n",
    "Now we can use our model to predict which type has a flower, basing on its parameters.\n",
    "\n",
    "This is conducted basically via traversing the tree that you can see above."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "predictions = clf.predict(X_test)"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### We can also measure the accuracy of our model"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "sum(predictions == y_test) / len(y_test)"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "To get clearer intuition about predicion, let's look at those X, that should be labeled to some flower"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "y_test"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "Here you can traverse the tree above by yourself and make sure that prediction works"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "X_test[1]"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "clf.predict([X_test[1]])"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "![purple-divider](https://user-images.githubusercontent.com/7065401/52071927-c1cd7100-2562-11e9-908a-dde91ba14e59.png)\n",
    "### Finally, it is your turn to write such classifier by yourself!\n",
    "\n",
    "####  Gini impurity\n",
    "\n",
    "Decision trees use the concept of Gini impurity to describe how “pure” a node is. A node is pure (G = 0) if all its samples belong to the same class, while a node with many samples from many different classes will have a Gini closer to 1.\n",
    "\n",
    "$G = 1 - \\sum_{k=1}^{n}p_{k}^2$\n",
    "\n",
    "For example, if a node contains five samples, with two belonging to the first class (first flower), two of class 2, one of class 3 and none of class 4, then\n",
    "\n",
    "$G = 1 - (\\frac{2}{5})^2 - (\\frac{2}{5})^2 - (\\frac{1}{5})^2 = 0.64$\n",
    "\n",
    "#### Remarks \n",
    "- We recommend using additional functions in `DecisionTreeClassifier` class, to make the implementation process easier.\n",
    "- [use this hint](https://arc.net/l/quote/pqvyjqei)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "a = [1,  2,  1,  1,   4, 43, 342, 2]\n",
    "b = [11, 2,  4,  554, 3, 6,  6,   2]\n",
    "a = np.array(a)\n",
    "b = np.array(b)\n",
    "sum(a == b) / len(b)"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-07T16:15:16.166523Z",
     "start_time": "2025-02-07T16:15:16.162523Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Node:\n",
    "    def __init__(self, X: npt.NDArray, y: npt.NDArray, feature_index: int=0, threshold:\n",
    "    float=0, left=None, right=None, value=None):\n",
    "        \"\"\"\n",
    "        :param X: numpy array of form [[feature1,feature2, ... featureN], ...] (i.e. [[1.5, 5.4, 3.2, 9.8] , ...] for case with iris d.s.)\n",
    "        :param y: numpy array of from [class1, class2, ...] (i.e. [0,1,1,2,1,0,...] for case with iris d.s.)\n",
    "        \"\"\"\n",
    "\n",
    "        self.value = value\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.feature_index = feature_index\n",
    "        self.threshold = threshold\n",
    "        self.left = left\n",
    "        self.right = right"
   ],
   "outputs": [],
   "execution_count": 46
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-07T16:15:18.120570Z",
     "start_time": "2025-02-07T16:15:18.112572Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class DecisionTreeClassifier:\n",
    "    def __init__(self, max_depth: int=5) -> None:\n",
    "        self.max_depth = max_depth\n",
    "        self.tree = None\n",
    "        self.number_of_classes = None\n",
    "\n",
    "    def fit(self, X: npt.NDArray, y: npt.NDArray) -> None:\n",
    "        \"\"\"\n",
    "        Basically, function that performs all the training (building of a tree)\n",
    "        We recommend to use it as a wrapper of recursive building function\n",
    "        \"\"\"\n",
    "        self.number_of_classes = np.unique(y).size\n",
    "        self.tree = self.build_tree(X, y)\n",
    "\n",
    "    def build_tree(self, X: npt.NDArray, y: npt.NDArray, depth=0):\n",
    "        sample_size, feature_size = X.shape\n",
    "        # print(sample_size, feature_size)\n",
    "        if sample_size > 1 and depth <= self.max_depth:\n",
    "            best_split = self.get_best_split(X, y, feature_size)\n",
    "            if best_split['IG'] > 0:\n",
    "                l_tree = self.build_tree(best_split['left_split'][:, :-1], best_split[\n",
    "                    'left_split'][:, -1], depth + 1)\n",
    "                r_tree = self.build_tree(best_split['right_split'][:, :-1], best_split[\n",
    "                    'right_split'][:, -1], depth + 1)\n",
    "                return Node(X, y, best_split['index'], best_split['threshold'], l_tree, r_tree)\n",
    "\n",
    "        return Node(X, y)\n",
    "\n",
    "    def get_best_split(self, X: npt.NDArray, y: npt.NDArray, feature_size: int)\\\n",
    "            -> dict:\n",
    "        best_split = {}\n",
    "        max_info_gain = float('-inf')\n",
    "\n",
    "        for feature_index in range(feature_size):\n",
    "            feature_values = X[:, feature_index]\n",
    "            unique_thresholds = np.unique(feature_values)\n",
    "            for threshold in unique_thresholds:\n",
    "                data = np.c_[X, y]\n",
    "                left_data, right_data = self.split(data, feature_index, threshold)\n",
    "                if len(left_data) <= 0 or len(right_data) <= 0:\n",
    "                    continue\n",
    "                left_y, right_y = left_data[:, -1], right_data[:, -1]\n",
    "                info_gain = self.calc_info_gain(y, left_y, right_y)\n",
    "\n",
    "                if info_gain > max_info_gain:\n",
    "                    best_split = {\n",
    "                        'index': feature_index,\n",
    "                        'threshold': threshold,\n",
    "                        'left_split': left_data,\n",
    "                        'right_split': right_data,\n",
    "                        'IG': info_gain\n",
    "                    }\n",
    "                    max_info_gain = info_gain\n",
    "        return best_split\n",
    "\n",
    "    def calc_info_gain(self, y, left_y, right_y):\n",
    "        left_weight = len(left_y) / len(y)\n",
    "        right_weight = len(right_y) / len(y)\n",
    "        return self.calc_entropy(y) - (left_weight * self.calc_entropy(left_y) + right_weight * self.calc_entropy(right_y))\n",
    "\n",
    "    @staticmethod\n",
    "    def calc_entropy(y):\n",
    "        y = np.unique(y, return_counts=True)[1]/len(y)\n",
    "        return np.sum(-y * np.log2(y))\n",
    "\n",
    "    @staticmethod\n",
    "    def split(data, feature_index, threshold):\n",
    "        dataset_left = np.array([row for row in data if row[feature_index] <= threshold])\n",
    "        dataset_right = np.array([row for row in data if row[feature_index] > threshold])\n",
    "        return dataset_left, dataset_right\n",
    "\n",
    "    def predict(self, X_test: npt.NDArray):\n",
    "        \"\"\"\n",
    "        Traverse the tree while there is a child\n",
    "        and return the predicted class for it\n",
    "        \"\"\"\n",
    "        return np.array([self.make_prediction(x, self.tree) for x in X_test])\n",
    "\n",
    "    @staticmethod\n",
    "    def calc_val(y):\n",
    "        y = list(y)\n",
    "        return max(y, key=y.count)\n",
    "\n",
    "    def make_prediction(self, x, tree):\n",
    "        if tree.left is None:\n",
    "            return self.calc_val(tree.y)\n",
    "        if x[tree.feature_index] <= tree.threshold:\n",
    "            return self.make_prediction(x, tree.left)\n",
    "        return self.make_prediction(x, tree.right)"
   ],
   "outputs": [],
   "execution_count": 47
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-07T16:15:22.946775Z",
     "start_time": "2025-02-07T16:15:22.943420Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "def evaluate(X_test: npt.NDArray, y_test: npt.NDArray):\n",
    "    \"\"\"\n",
    "    Returns accuracy of the model (ratio of right guesses to the number of samples)\n",
    "    \"\"\"\n",
    "    print(f'The tree acuracy is: {sum(X_test == y_test) / len(X_test) * 100:.2f}%')"
   ],
   "outputs": [],
   "execution_count": 48
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-07T19:49:35.728154Z",
     "start_time": "2025-02-07T19:49:35.714152Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# X, y = iris.data, iris.target\n",
    "cancer = load_breast_cancer()\n",
    "X, y = cancer.data, cancer.target\n",
    "\n",
    "X, X_test, y, y_test = train_test_split(X, y, test_size= 0.20)"
   ],
   "outputs": [],
   "execution_count": 107
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-07T19:49:48.579565Z",
     "start_time": "2025-02-07T19:49:48.564658Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "tree = DecisionTreeClassifier(max_depth=10)\n",
    "tree.fit(X, y)\n",
    "evaluate(tree.predict(X_test), y_test)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tree acuracy is: 92.11%\n"
     ]
    }
   ],
   "execution_count": 109
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
